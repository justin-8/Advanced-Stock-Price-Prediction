{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKMP3h0XU2ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuiKCq3hU2cd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJqlAdkHU2ce",
    "outputId": "5278f1be-d005-4681-f0ee-44a5c8dd55cb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXpPyzeVU2cf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas_ta as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uudw7IvsU2cg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzJktkK7U2cg",
    "outputId": "585759a8-63be-42f5-a674-49d8bcb99ee2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Everything concerning USDTRY pair is done here, after testing the models CHECK THIS PART!\n",
    "bist_daily = pd.read_csv(\"datasets/bist_100_daily.csv\")\n",
    "usdtry_daily = pd.read_csv('datasets/USDTRY_daily.csv')\n",
    "usdtry_daily = usdtry_daily[[\"Date\",\"USDTRY_Close\"]]\n",
    "usdtry_daily[\"Date\"]=pd.to_datetime(usdtry_daily[\"Date\"])\n",
    "bist_daily[\"Date\"] = pd.to_datetime(bist_daily[\"Date\"])\n",
    "bist_daily = bist_daily.merge(usdtry_daily, how='inner',on=\"Date\")\n",
    "\n",
    "\n",
    "# Extract the number of rows and columns by using the shape of the data.\n",
    "numRows,numColumns = bist_daily.shape\n",
    "# Extract the time interval.\n",
    "last_date, first_date = bist_daily.iloc[0].Date, bist_daily.iloc[-1].Date\n",
    "# Check the availability of the data.\n",
    "na_cols = bist_daily.columns[bist_daily.isna().any()].tolist()\n",
    "\n",
    "# Print the information.\n",
    "print(f\"There are {numRows} rows and {numColumns} columns in the initial dataset.\")\n",
    "print(f\"The data represents the time frame between the dates '{last_date}' and '{first_date}'.\")\n",
    "if not na_cols:\n",
    "    print(\"There are no NA rows.\")\n",
    "else:\n",
    "    print(f\"Columns in the dataset which include NA rows: {na_cols}.\")\n",
    "# Convert columns to numeric values\n",
    "column_names = [\"Price\", \"Open\", \"High\", \"Low\"]\n",
    "for column in column_names:\n",
    "    bist_daily[column] = bist_daily[column].str.replace(',', '')\n",
    "    bist_daily[column] = pd.to_numeric(bist_daily[column])\n",
    "# CONVERT TO DATETIME FORMAT AND SORT DATA BY DATE\n",
    "bist_daily.Date = pd.to_datetime(bist_daily.Date)\n",
    "bist_daily.sort_values(by=\"Date\", ignore_index=True,inplace=True)\n",
    "bist_daily.set_index(pd.DatetimeIndex(bist_daily[\"Date\"]), inplace=True)\n",
    "bist_daily.rename(columns={\"Price\": \"close\"},inplace=True)\n",
    "# Calculate Returns and append to the df DataFrame\n",
    "# CUMLOGRET_1 and CUMPCTRET_1 are added (NaN values exists)\n",
    "bist_daily.ta.log_return(cumulative=True, append=True)\n",
    "bist_daily.ta.percent_return(cumulative=True, append=True)\n",
    "# Returns a list of indicators and utility functions (to check in future)\n",
    "ind_list = bist_daily.ta.indicators(as_list=True)\n",
    "# RSI_14, MACD_12_26_9, MACDh_12_26_9 and MACDs_12_26_9 are added (NaN values exists)\n",
    "bist_daily.ta.rsi(append=True)\n",
    "bist_daily.ta.macd(append=True)\n",
    "# SMA values are added (use ta in the future)\n",
    "sma_values = [5, 10, 15] \n",
    "for i in sma_values:\n",
    "    bist_daily['SMA'+str(i)] = bist_daily['close'].rolling(window=i).mean()\n",
    "# Remove all NaN value rows\n",
    "bist_daily.dropna(inplace=True)\n",
    "bist_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bStKHxQVU2ci",
    "outputId": "83379387-c90b-4ce0-a61c-a2ec52459803"
   },
   "outputs": [],
   "source": [
    "# Visualize the correlation matrix.\n",
    "fig = plt.figure(figsize=(10,10)) \n",
    "ax = fig.add_subplot(111) # 1x1 grid, first subplot.\n",
    "fig.colorbar(ax.matshow(bist_daily.corr(), vmin=-1, vmax=1)) \n",
    "ticks = np.arange(len(bist_daily.corr().columns))\n",
    "ax.tick_params(axis='x', which='major', labelsize=14, rotation=90)\n",
    "ax.tick_params(axis='y', which='major', labelsize=14, rotation=0)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(bist_daily.corr().columns)\n",
    "ax.set_yticklabels(bist_daily.corr().columns)\n",
    "plt.title(\"Correlation Matrix\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSjmo0WiU2ci",
    "outputId": "9d1b9f9d-133b-45e3-db39-72dca7c338dc"
   },
   "outputs": [],
   "source": [
    "diff = [] \n",
    "for date1, date2 in zip(bist_daily[\"Date\"][:-1], bist_daily[\"Date\"][1:]): \n",
    "    diff.append((date2-date1).days) \n",
    "unique, counts = np.unique(np.asarray(diff), return_counts=True) \n",
    "count_dict = dict(zip(unique, counts)) \n",
    "print(count_dict) \n",
    "plt.bar(unique,counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPm6igNhU2cj",
    "outputId": "128847b4-31a4-43dd-c28e-14017f64ca85"
   },
   "outputs": [],
   "source": [
    "# edit Vol. column\n",
    "from operator import itemgetter\n",
    "\n",
    "vols = bist_daily['Vol.'].to_list()\n",
    "#[vol[-1] for vol in vols]\n",
    "#list(filter(lambda vol: \"-\" in vol,enumerate(vols)))\n",
    "\n",
    "indexToRemove = bist_daily.iloc[list(map(itemgetter(0),filter(lambda vol: \"-\" in vol,enumerate(vols))))].index\n",
    "bist_daily.drop(indexToRemove,inplace=True)\n",
    "bist_daily['Vol.'] = bist_daily['Vol.'].apply(\n",
    "    lambda x: float(x[:-1])*(10**6) if x[-1]==\"M\" else (float(x[:-1])*(10**9) if x[-1]==\"B\" else \"ERROR\")).astype(\"int\")\n",
    "bist_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iv6C3N19U2ck",
    "outputId": "c391721f-d684-42ec-d95a-60e7d080b930",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seeding an arbitrary number to get results in multiple runs\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "print(\"Seed:\", manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuDbW8ZpU2ck",
    "outputId": "56c17b58-2fe9-46ca-f29a-cac9edf6315a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting number of GPUs from cuda\n",
    "ngpu = torch.cuda.device_count()\n",
    "print(\"Count of available GPUs:\", ngpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZriUaU5-U2ck",
    "outputId": "aa4d0040-9bcb-4094-f60d-35c5ad80ab9f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# printing the name of available GPUs\n",
    "for i in range(ngpu):\n",
    "    print(\"GPU {}: {}\".format(i+1, torch.cuda.get_device_name(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wToWHJ64U2cl",
    "outputId": "b08b060d-a4fe-4e7d-b68f-e4f2eece74d6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch size for the training\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer rates\n",
    "optimizer_betas = (0.9, 0.999)\n",
    "learning_rate = 5.125e-4\n",
    "\n",
    "# number of epochs\n",
    "num_epochs = 100000\n",
    "\n",
    "# evaluate after evaluation_epoch_num epochs\n",
    "evaluation_epoch_num = 500\n",
    "\n",
    "# decide which device we want to run on\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUonmzZ9U2cl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, data_frame, sequence_length=2):\n",
    "        self.data = torch.tensor(data_frame.values)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index: index + self.sequence_length].float()\n",
    "    \n",
    "    # Non-overlapping series\n",
    "    # def __getitem__(self, index):\n",
    "    #     return self.data[index * self.sequence_length: (index+1) * self.sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02hP7PWXU2cl",
    "outputId": "e88fb7ad-cb7c-4ef5-a42f-8017d909a309",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create pytorch dataset from the pandas DataFrame\n",
    "\n",
    "# TODO: Convert change(%) and Volume columns to numeric values\n",
    "columns_used_in_training = [\"close\", \"Open\", \"High\", \"Low\", \"CUMLOGRET_1\", \"RSI_14\", \"MACD_12_26_9\", \"SMA5\"]\n",
    "# input dimension of the generator\n",
    "data_dimension = len(columns_used_in_training)\n",
    "# sequence length of input data\n",
    "sequence_length = 7\n",
    "\n",
    "train_data, rest_data = train_test_split(bist_daily[columns_used_in_training], test_size=0.2, shuffle=False)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data[train_data.columns] = scaler.transform(train_data)\n",
    "rest_data[rest_data.columns] = scaler.transform(rest_data)\n",
    "\n",
    "validation_data, test_data = train_test_split(rest_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "train_dataset = TimeseriesDataset(train_data, sequence_length)\n",
    "test_dataset = TimeseriesDataset(test_data, sequence_length)\n",
    "validation_dataset = TimeseriesDataset(validation_data, sequence_length)\n",
    "\n",
    "# create the dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size)\n",
    "real_data_sample = next(iter(train_dataloader))\n",
    "print(\"Real data sample shape:\", real_data_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsqSOoShU2cm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=data_dimension, hidden_size=hidden_size, num_layers=1, dropout=0.2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, data_dimension)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input_sequences):\n",
    "        input_sequences = self.drop(input_sequences)\n",
    "        lstm_output, hidden_cell = self.lstm(input_sequences)\n",
    "        res = self.linear(hidden_cell[0][-1])\n",
    "        res = res.view(res.shape[0], 1, -1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uK3_aNVyU2cm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=data_dimension, hidden_size=hidden_size, num_layers=1, dropout=0.2, batch_first=True)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input_sequences):\n",
    "        input_sequences = self.drop(input_sequences)\n",
    "        lstm_output, hidden_cell = self.lstm(input_sequences)\n",
    "        res = self.linear(hidden_cell[0][-1])\n",
    "        res = res.view(res.shape[0], 1, -1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZFDZS--U2cm"
   },
   "outputs": [],
   "source": [
    "def model_rmse(model, dataloader, epoch, plot_graph=False, plot_title=\"Validation Predictions\", show_preds=False):\n",
    "    rmse = 0\n",
    "    squared_error_list = []\n",
    "    real_data_list = []\n",
    "    predicted_data_list = []\n",
    "    file_title = plot_title.lower().replace(\" \", \"_\")\n",
    "    for i, sequence_batch in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            real_sequence = sequence_batch\n",
    "            # Assign first t values\n",
    "            generator_input_sequence = sequence_batch[:,:-1].to(device)\n",
    "            real_values = sequence_batch[:,-1:]\n",
    "            #  Generate (t+1)th value from first t values\n",
    "            predicted_values = generator(generator_input_sequence).cpu()\n",
    "            real_data_list.append(real_values)\n",
    "            predicted_data_list.append(predicted_values)\n",
    "    \n",
    "    real_data = torch.cat(real_data_list, 0)\n",
    "    predicted_data = torch.cat(predicted_data_list, 0)\n",
    "    \n",
    "    # Unscale data\n",
    "    df_pred = pd.DataFrame(predicted_data.view(-1,len(columns_used_in_training)),columns=columns_used_in_training)\n",
    "    df_pred_unscaled = pd.DataFrame(scaler.inverse_transform(df_pred),columns=columns_used_in_training)\n",
    "    df_real = pd.DataFrame(real_data.view(-1,len(columns_used_in_training)),columns=columns_used_in_training)\n",
    "    df_real_unscaled = pd.DataFrame(scaler.inverse_transform(df_real),columns=columns_used_in_training)\n",
    "    \n",
    "    if plot_graph:\n",
    "        if not os.path.exists('./plots_lstm_disc/'):\n",
    "            os.makedirs('./plots_lstm_disc/')\n",
    "        \n",
    "        for column in columns_used_in_training:\n",
    "            #get x values and plot prediction of multiple columns\n",
    "            fig = plt.figure(figsize=(16,8))\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(column)\n",
    "            plt.title(plot_title + f\" -{column}-\")\n",
    "            plt.plot(df_real_unscaled[column],label=\"Real\")\n",
    "            plt.plot(df_pred_unscaled[column],label=\"Predicted\")\n",
    "            # plt.ylim(bottom=0)\n",
    "            plt.legend()\n",
    "            if show_preds and column == \"close\":\n",
    "                plt.show()\n",
    "            fig.savefig(f'./plots_lstm_disc/{file_title}_plt_{column}_e{epoch}.png')\n",
    "            plt.close(fig)\n",
    "    rmse_results = {}\n",
    "    for column in columns_used_in_training:\n",
    "        squared_errors = (df_real_unscaled[column] - df_pred_unscaled[column])**2\n",
    "        rmse = np.sqrt(squared_errors.mean())\n",
    "        rmse_results[column] = rmse\n",
    "    return rmse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zY1LjDeLU2cn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weight initialization of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErRiVwquU2cn",
    "outputId": "91265213-439c-418c-9abb-a529bfc29f7e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator = Generator(hidden_size=data_dimension*2).to(device)\n",
    "discriminator = Discriminator(hidden_size=data_dimension*2).to(device)\n",
    "print(\"Generator and discriminator are initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzAxXEx4U2cn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=learning_rate, betas=optimizer_betas)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=optimizer_betas)\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEwcd__HU2cn"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./models_lstm_disc/'):\n",
    "    os.makedirs('./models_lstm_disc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ualip0WJU2co",
    "outputId": "5dd00301-2803-4314-dc3d-8fbc42896dda"
   },
   "outputs": [],
   "source": [
    "best_predictor = None\n",
    "min_close_rmse = math.inf\n",
    "\n",
    "evaluation_metrics = {\"gen_loss\":[], \"disc_loss\":[], \"rmse_values\":{}}\n",
    "for column in columns_used_in_training:\n",
    "        evaluation_metrics[\"rmse_values\"][column] = []\n",
    "                      \n",
    "print(\"Training is started\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, sequence_batch in enumerate(train_dataloader):\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            ## Training with real batch\n",
    "            discriminator.zero_grad()\n",
    "            # Format batch\n",
    "            real_sequence = sequence_batch.to(device)\n",
    "            batch_size = real_sequence.size(0)\n",
    "            real_labels = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "            # Forward pass real batch through D\n",
    "            discriminator_output_real = discriminator(real_sequence).view(-1)\n",
    "            # Calculate loss on all-real batch\n",
    "            discriminator_error_real = criterion(discriminator_output_real, real_labels)\n",
    "            # Calculate gradients for D in backward pass\n",
    "            discriminator_error_real.backward()\n",
    "\n",
    "            ## Training with fake batch\n",
    "            # Assign first t values\n",
    "            generator_input_sequence = sequence_batch[:,:-1].to(device)\n",
    "            #  Generate (t+1)th value from first t values\n",
    "            generated_values = generator(generator_input_sequence)\n",
    "            fake_labels = torch.full((batch_size,), fake_label, dtype=torch.float, device=device)\n",
    "            # Concat first t real values and generated (t+1)th values\n",
    "            generator_result_concat = torch.cat((generator_input_sequence, generated_values.detach()), 1)\n",
    "            # Classify all fake batch with D\n",
    "            discriminator_output_fake = discriminator(generator_result_concat).view(-1)\n",
    "            # Calculate D's loss on the all-fake batch\n",
    "            discriminator_error_fake = criterion(discriminator_output_fake, fake_labels)\n",
    "            # Calculate the gradients for this batch\n",
    "            discriminator_error_fake.backward()\n",
    "            # Add the gradients from the all-real and all-fake batches\n",
    "            discriminator_error = discriminator_error_real + discriminator_error_fake\n",
    "            # Update D\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            generator.zero_grad()\n",
    "            real_labels = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            generator_result_concat_grad = torch.cat((generator_input_sequence, generated_values), 1)\n",
    "            discriminator_output_fake = discriminator(generator_result_concat_grad).view(-1)\n",
    "            # Calculate G's loss based on this output\n",
    "            generator_error = criterion(discriminator_output_fake, real_labels)\n",
    "            # Calculate gradients for G\n",
    "            generator_error.backward()\n",
    "            # Update G\n",
    "            optimizer_generator.step()\n",
    "    if (epoch+1) % evaluation_epoch_num == 0 or epoch+1 == 1:\n",
    "        rmse_values = model_rmse(generator, validation_dataloader, epoch=(epoch+1), plot_graph=True)\n",
    "        if rmse_values[\"close\"] < min_close_rmse:\n",
    "            min_close_rmse = rmse_values[\"close\"]\n",
    "            best_predictor = epoch+1\n",
    "        for column in columns_used_in_training:\n",
    "            evaluation_metrics[\"rmse_values\"][column].append(rmse_values[column])\n",
    "        evaluation_metrics[\"gen_loss\"].append(generator_error.item())\n",
    "        evaluation_metrics[\"disc_loss\"].append(discriminator_error.item())\n",
    "        print('\\n[{}/{}]\\tDiscriminator Loss: {:.4f}\\tGenerator Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, discriminator_error.item(), generator_error.item()))\n",
    "        for col_name, rmse in rmse_values.items():\n",
    "            print(f\"{col_name} RMSE: {rmse:.4f}\")\n",
    "        save_path = os.path.join(\"./models_lstm_disc/\",\"model_epoch_{}.pt\".format(epoch+1))\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'generator_model_state_dict': generator.state_dict(),\n",
    "            'discriminator_model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
    "            'optimizer_discriminator_state_dict': optimizer_discriminator.state_dict(),\n",
    "            'discriminator_loss': discriminator_error,\n",
    "            'generator_loss': generator_error,\n",
    "            }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hp3jahWlU2co",
    "outputId": "b2fbbd6e-6b0a-4913-d75b-21a9095b508f"
   },
   "outputs": [],
   "source": [
    "for key, val in evaluation_metrics[\"rmse_values\"].items():\n",
    "    print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WOazn8gU2co",
    "outputId": "2ffdae0d-d0d6-4f14-be70-5a0bcf1166f2"
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for epoch in range(num_epochs):\n",
    "    if (epoch+1)%evaluation_epoch_num == 0 or epoch == 0 or epoch == (num_epochs-1):\n",
    "        image_path = os.path.join('./plots_lstm_disc/', f'validation_predictions_plt_close_e{epoch+1}.png')\n",
    "        image = plt.imread(image_path)\n",
    "        images.append(image)\n",
    "#%%capture\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "all_images = [[plt.imshow(image, animated=True)] for image in images]\n",
    "plt.rcParams['animation.embed_limit'] = 2**32\n",
    "ani = animation.ArtistAnimation(fig, all_images, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92Weu8P5U2cp",
    "outputId": "c464c4f2-8180-4e34-f4a6-91698659d5b4"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(64,32))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"RMSE Values\")\n",
    "epoch_len = len(evaluation_metrics[\"rmse_values\"][columns_used_in_training[0]])\n",
    "x = [0] + [i for i in range(evaluation_epoch_num, evaluation_epoch_num*epoch_len, evaluation_epoch_num)]\n",
    "# plt.ylim(0, 5)\n",
    "for column in columns_used_in_training:\n",
    "    y = evaluation_metrics[\"rmse_values\"][column]\n",
    "    y /= max(y)\n",
    "    plt.plot(x, y,label=column)\n",
    "plt.legend()\n",
    "fig.savefig(f'./plots_lstm_disc/rmse_all.png')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmVlwHvuU2cp",
    "outputId": "9c1309e7-224e-41e2-f6ea-901fb2d5df28"
   },
   "outputs": [],
   "source": [
    "for column in columns_used_in_training:\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(f\"RMSE {column}\")\n",
    "    epoch_len = len(evaluation_metrics[\"rmse_values\"][columns_used_in_training[0]])\n",
    "    x = [0] + [i for i in range(evaluation_epoch_num, evaluation_epoch_num*epoch_len, evaluation_epoch_num)]\n",
    "    # plt.ylim(0, 5)\n",
    "    y = evaluation_metrics[\"rmse_values\"][column]\n",
    "    y /= max(y)\n",
    "    plt.plot(x, y,label=\"RMSE\")\n",
    "    plt.legend()\n",
    "    fig.savefig(f'./plots_lstm_disc/rmse_{column}.png')\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TxGLW5WU2cp",
    "outputId": "b6d1be41-f311-4652-adfc-f499ec93c0f0"
   },
   "outputs": [],
   "source": [
    "for column in columns_used_in_training:\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(f\"RMSE {column} (After Epoch 5000)\")\n",
    "    epoch_len = len(evaluation_metrics[\"rmse_values\"][columns_used_in_training[0]])\n",
    "    x = [0] + [i for i in range(evaluation_epoch_num, evaluation_epoch_num*epoch_len, evaluation_epoch_num)]\n",
    "    # plt.ylim(0, 5)\n",
    "    y = evaluation_metrics[\"rmse_values\"][column]\n",
    "    y /= max(y)\n",
    "    plt.plot(x[10:], y[10:],label=\"RMSE\")\n",
    "    plt.legend()\n",
    "    fig.savefig(f'./plots_lstm_disc/rmse_{column}.png')\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1RqJsGKU2cp",
    "outputId": "e9845688-1b3d-44cb-9aed-006c7cfa2beb"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(\"./models_lstm_disc/\",\"model_epoch_{}.pt\".format(best_predictor)))\n",
    "generator.load_state_dict(checkpoint['generator_model_state_dict'])\n",
    "rmse_values = model_rmse(generator, test_dataloader, epoch=best_predictor, plot_graph=True, plot_title=\"Test Predictions\", show_preds=True)\n",
    "print(rmse_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJhdn9J3U2cp",
    "outputId": "6f5b8695-8d67-4eaa-e239-8da8255641fc"
   },
   "outputs": [],
   "source": [
    "best_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p53ptxFoU2cp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDxVLNgtU2cq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "smpp_gan_lstm_disc.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
